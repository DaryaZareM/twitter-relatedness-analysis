{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "in_reply_to_user_id\n",
      "is_quote\n",
      "is_retweet\n",
      "like_count\n",
      "quote_count\n",
      "quoted_text\n",
      "reply_count\n",
      "retweet_count\n",
      "retweet_text\n",
      "text\n",
      "urls_expanded_url\n",
      "urls_url\n",
      "relevant\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('data.csv')\n",
    "for i in dataset.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>quoted_text</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>urls_expanded_url</th>\n",
       "      <th>urls_url</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.140000e+17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@OzraeliAvi @SydneyLWatson @RitaPanahi @thejui...</td>\n",
       "      <td>https://www.theguardian.com/australia-news/201...</td>\n",
       "      <td>https://t.co/5AGrpxapqB</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.574775e+08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Red_dragon_fly @warr_cameron @NSWRFS Tell me ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we just have a fucking blanket ban on cele...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.429786e+07</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@scottsantens Scott, üôè  for the tweets re: And...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.229958e+08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Javedakhtarjadu Misleading information someti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>7594</td>\n",
       "      <td>1.816678e+07</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Jim_Jordan Learn about Viruses, Vaccines, Soc...</td>\n",
       "      <td>http://TheBigVirusHoax.com</td>\n",
       "      <td>https://t.co/dujlhndLDR</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>7595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDITORIAL: Say no to Trudeau's online censorsh...</td>\n",
       "      <td>https://torontosun.com/opinion/editorials/edit...</td>\n",
       "      <td>https://t.co/XHIRTa33yL</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>7596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hate trump too but at least he ain‚Äôt try to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>7597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DV_Reporter @immcouncil @POTUS @HouseDemocrat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>please HELP and SUPPORT us\\nWe are the biggest...</td>\n",
       "      <td>https://twitter.com/Elham40686511/status/13879...</td>\n",
       "      <td>https://t.co/GKSe2f6SiC</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>7598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PFA hits out at Twitter for not taking down ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They‚Äôll ban Donald Trump to virtue signal thou...</td>\n",
       "      <td>https://twitter.com/markogden_/status/13878857...</td>\n",
       "      <td>https://t.co/mjYqcBXiZd</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7599 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  in_reply_to_user_id  is_quote  is_retweet  like_count  \\\n",
       "0        0         9.140000e+17     False       False           0   \n",
       "1        1         2.574775e+08     False       False           2   \n",
       "2        2                  NaN     False       False           0   \n",
       "3        3         1.429786e+07     False       False           0   \n",
       "4        4         1.229958e+08     False       False           0   \n",
       "...    ...                  ...       ...         ...         ...   \n",
       "7594  7594         1.816678e+07     False       False           0   \n",
       "7595  7595                  NaN     False       False           0   \n",
       "7596  7596                  NaN     False       False           0   \n",
       "7597  7597                  NaN      True       False           0   \n",
       "7598  7598                  NaN      True       False           1   \n",
       "\n",
       "      quote_count                                        quoted_text  \\\n",
       "0               0                                                NaN   \n",
       "1               0                                                NaN   \n",
       "2               0                                                NaN   \n",
       "3               0                                                NaN   \n",
       "4               0                                                NaN   \n",
       "...           ...                                                ...   \n",
       "7594            0                                                NaN   \n",
       "7595            0                                                NaN   \n",
       "7596            0                                                NaN   \n",
       "7597            0  @DV_Reporter @immcouncil @POTUS @HouseDemocrat...   \n",
       "7598            0  PFA hits out at Twitter for not taking down ra...   \n",
       "\n",
       "      reply_count  retweet_count  retweet_text  \\\n",
       "0               1              0           NaN   \n",
       "1               1              0           NaN   \n",
       "2               0              0           NaN   \n",
       "3               0              0           NaN   \n",
       "4               0              0           NaN   \n",
       "...           ...            ...           ...   \n",
       "7594            0              0           NaN   \n",
       "7595            0              0           NaN   \n",
       "7596            0              0           NaN   \n",
       "7597            0              0           NaN   \n",
       "7598            0              1           NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0     @OzraeliAvi @SydneyLWatson @RitaPanahi @thejui...   \n",
       "1     @Red_dragon_fly @warr_cameron @NSWRFS Tell me ...   \n",
       "2     Can we just have a fucking blanket ban on cele...   \n",
       "3     @scottsantens Scott, üôè  for the tweets re: And...   \n",
       "4     @Javedakhtarjadu Misleading information someti...   \n",
       "...                                                 ...   \n",
       "7594  @Jim_Jordan Learn about Viruses, Vaccines, Soc...   \n",
       "7595  EDITORIAL: Say no to Trudeau's online censorsh...   \n",
       "7596  I hate trump too but at least he ain‚Äôt try to ...   \n",
       "7597  please HELP and SUPPORT us\\nWe are the biggest...   \n",
       "7598  They‚Äôll ban Donald Trump to virtue signal thou...   \n",
       "\n",
       "                                      urls_expanded_url  \\\n",
       "0     https://www.theguardian.com/australia-news/201...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "7594                         http://TheBigVirusHoax.com   \n",
       "7595  https://torontosun.com/opinion/editorials/edit...   \n",
       "7596                                                NaN   \n",
       "7597  https://twitter.com/Elham40686511/status/13879...   \n",
       "7598  https://twitter.com/markogden_/status/13878857...   \n",
       "\n",
       "                     urls_url    relevant  \n",
       "0     https://t.co/5AGrpxapqB    Relevant  \n",
       "1                         NaN    Relevant  \n",
       "2                         NaN    Relevant  \n",
       "3                         NaN    Relevant  \n",
       "4                         NaN  Irrelevant  \n",
       "...                       ...         ...  \n",
       "7594  https://t.co/dujlhndLDR    Relevant  \n",
       "7595  https://t.co/XHIRTa33yL    Relevant  \n",
       "7596                      NaN  Irrelevant  \n",
       "7597  https://t.co/GKSe2f6SiC  Irrelevant  \n",
       "7598  https://t.co/mjYqcBXiZd  Irrelevant  \n",
       "\n",
       "[7599 rows x 14 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #taking care of NaN datas\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "# imputer.fit(X)\n",
    "# X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \n",
    "    #import string\n",
    "    punctuation = '[!-.:?;\"\\n\"()''\"\",_%$\\|/,<>‚Äô‚Äò]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    \n",
    "    #remove username with a @ and remove the link which start with http\n",
    "    juntext = \" \".join(filter(lambda x:x[0]!='@', text_subbed.split()))\n",
    "    juntext = \" \".join(filter(lambda x:not 'http' in x, juntext.split()))\n",
    "    \n",
    "    return juntext.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_text = dataset[['id','text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans up text\n",
    "id_text['text'] = id_text['text'].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the guardian had the balls to ignore the media...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tell me how a permanent fire ban helps anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>can we just have a fucking blanket ban on cele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>scott üôè for the tweets re andrew yang im not f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>misleading information sometimes close the fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>7594</td>\n",
       "      <td>learn about viruses vaccines social media cens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>7595</td>\n",
       "      <td>editorial say no to trudeaus online censorship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>7596</td>\n",
       "      <td>i hate trump too but at least he aint try to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>7597</td>\n",
       "      <td>please help and support uswe are the biggest v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>7598</td>\n",
       "      <td>theyll ban donald trump to virtue signal thoug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7599 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0        0  the guardian had the balls to ignore the media...\n",
       "1        1  tell me how a permanent fire ban helps anyone ...\n",
       "2        2  can we just have a fucking blanket ban on cele...\n",
       "3        3  scott üôè for the tweets re andrew yang im not f...\n",
       "4        4  misleading information sometimes close the fre...\n",
       "...    ...                                                ...\n",
       "7594  7594  learn about viruses vaccines social media cens...\n",
       "7595  7595  editorial say no to trudeaus online censorship...\n",
       "7596  7596  i hate trump too but at least he aint try to b...\n",
       "7597  7597  please help and support uswe are the biggest v...\n",
       "7598  7598  theyll ban donald trump to virtue signal thoug...\n",
       "\n",
       "[7599 rows x 2 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "#applying function to the column\n",
    "id_text['msg_tokenied']= id_text['text'].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nlp library\n",
    "import nltk\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "#applying the function\n",
    "id_text['msg_tokenied']= id_text['msg_tokenied'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>msg_tokenied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the guardian had the balls to ignore the media...</td>\n",
       "      <td>[guardian, balls, ignore, media, ban, stopped,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tell me how a permanent fire ban helps anyone ...</td>\n",
       "      <td>[tell, permanent, fire, ban, helps, anyone, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>can we just have a fucking blanket ban on cele...</td>\n",
       "      <td>[fucking, blanket, ban, celebs, tv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>scott üôè for the tweets re andrew yang im not f...</td>\n",
       "      <td>[scott, üôè, tweets, andrew, yang, im, formal, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>misleading information sometimes close the fre...</td>\n",
       "      <td>[misleading, information, sometimes, close, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>7594</td>\n",
       "      <td>learn about viruses vaccines social media cens...</td>\n",
       "      <td>[learn, viruses, vaccines, social, media, cens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>7595</td>\n",
       "      <td>editorial say no to trudeaus online censorship...</td>\n",
       "      <td>[editorial, say, trudeaus, online, censorship,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>7596</td>\n",
       "      <td>i hate trump too but at least he aint try to b...</td>\n",
       "      <td>[hate, trump, least, aint, try, ban, backwoods...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>7597</td>\n",
       "      <td>please help and support uswe are the biggest v...</td>\n",
       "      <td>[please, help, support, uswe, biggest, victims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>7598</td>\n",
       "      <td>theyll ban donald trump to virtue signal thoug...</td>\n",
       "      <td>[theyll, ban, donald, trump, virtue, signal, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7599 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  the guardian had the balls to ignore the media...   \n",
       "1        1  tell me how a permanent fire ban helps anyone ...   \n",
       "2        2  can we just have a fucking blanket ban on cele...   \n",
       "3        3  scott üôè for the tweets re andrew yang im not f...   \n",
       "4        4  misleading information sometimes close the fre...   \n",
       "...    ...                                                ...   \n",
       "7594  7594  learn about viruses vaccines social media cens...   \n",
       "7595  7595  editorial say no to trudeaus online censorship...   \n",
       "7596  7596  i hate trump too but at least he aint try to b...   \n",
       "7597  7597  please help and support uswe are the biggest v...   \n",
       "7598  7598  theyll ban donald trump to virtue signal thoug...   \n",
       "\n",
       "                                           msg_tokenied  \n",
       "0     [guardian, balls, ignore, media, ban, stopped,...  \n",
       "1     [tell, permanent, fire, ban, helps, anyone, wo...  \n",
       "2                   [fucking, blanket, ban, celebs, tv]  \n",
       "3     [scott, üôè, tweets, andrew, yang, im, formal, s...  \n",
       "4     [misleading, information, sometimes, close, fr...  \n",
       "...                                                 ...  \n",
       "7594  [learn, viruses, vaccines, social, media, cens...  \n",
       "7595  [editorial, say, trudeaus, online, censorship,...  \n",
       "7596  [hate, trump, least, aint, try, ban, backwoods...  \n",
       "7597  [please, help, support, uswe, biggest, victims...  \n",
       "7598  [theyll, ban, donald, trump, virtue, signal, t...  \n",
       "\n",
       "[7599 rows x 3 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "id_text['msg_stemmed']=id_text['msg_tokenied'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>msg_tokenied</th>\n",
       "      <th>msg_stemmed</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the guardian had the balls to ignore the media...</td>\n",
       "      <td>[guardian, balls, ignore, media, ban, stopped,...</td>\n",
       "      <td>[guardian, ball, ignor, media, ban, stop, shor...</td>\n",
       "      <td>[guardian, ball, ignore, medium, ban, stopped,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tell me how a permanent fire ban helps anyone ...</td>\n",
       "      <td>[tell, permanent, fire, ban, helps, anyone, wo...</td>\n",
       "      <td>[tell, perman, fire, ban, help, anyon, would, ...</td>\n",
       "      <td>[tell, permanent, fire, ban, help, anyone, wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>can we just have a fucking blanket ban on cele...</td>\n",
       "      <td>[fucking, blanket, ban, celebs, tv]</td>\n",
       "      <td>[fuck, blanket, ban, celeb, tv]</td>\n",
       "      <td>[fucking, blanket, ban, celebs, tv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>scott üôè for the tweets re andrew yang im not f...</td>\n",
       "      <td>[scott, üôè, tweets, andrew, yang, im, formal, s...</td>\n",
       "      <td>[scott, üôè, tweet, andrew, yang, im, formal, su...</td>\n",
       "      <td>[scott, üôè, tweet, andrew, yang, im, formal, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>misleading information sometimes close the fre...</td>\n",
       "      <td>[misleading, information, sometimes, close, fr...</td>\n",
       "      <td>[mislead, inform, sometim, close, free, though...</td>\n",
       "      <td>[misleading, information, sometimes, close, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>7594</td>\n",
       "      <td>learn about viruses vaccines social media cens...</td>\n",
       "      <td>[learn, viruses, vaccines, social, media, cens...</td>\n",
       "      <td>[learn, virus, vaccin, social, media, censorsh...</td>\n",
       "      <td>[learn, virus, vaccine, social, medium, censor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>7595</td>\n",
       "      <td>editorial say no to trudeaus online censorship...</td>\n",
       "      <td>[editorial, say, trudeaus, online, censorship,...</td>\n",
       "      <td>[editori, say, trudeau, onlin, censorship, plan]</td>\n",
       "      <td>[editorial, say, trudeaus, online, censorship,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>7596</td>\n",
       "      <td>i hate trump too but at least he aint try to b...</td>\n",
       "      <td>[hate, trump, least, aint, try, ban, backwoods...</td>\n",
       "      <td>[hate, trump, least, aint, tri, ban, backwood,...</td>\n",
       "      <td>[hate, trump, least, aint, try, ban, backwoods...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>7597</td>\n",
       "      <td>please help and support uswe are the biggest v...</td>\n",
       "      <td>[please, help, support, uswe, biggest, victims...</td>\n",
       "      <td>[pleas, help, support, usw, biggest, victim, m...</td>\n",
       "      <td>[please, help, support, uswe, biggest, victim,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>7598</td>\n",
       "      <td>theyll ban donald trump to virtue signal thoug...</td>\n",
       "      <td>[theyll, ban, donald, trump, virtue, signal, t...</td>\n",
       "      <td>[theyll, ban, donald, trump, virtu, signal, th...</td>\n",
       "      <td>[theyll, ban, donald, trump, virtue, signal, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7599 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  the guardian had the balls to ignore the media...   \n",
       "1        1  tell me how a permanent fire ban helps anyone ...   \n",
       "2        2  can we just have a fucking blanket ban on cele...   \n",
       "3        3  scott üôè for the tweets re andrew yang im not f...   \n",
       "4        4  misleading information sometimes close the fre...   \n",
       "...    ...                                                ...   \n",
       "7594  7594  learn about viruses vaccines social media cens...   \n",
       "7595  7595  editorial say no to trudeaus online censorship...   \n",
       "7596  7596  i hate trump too but at least he aint try to b...   \n",
       "7597  7597  please help and support uswe are the biggest v...   \n",
       "7598  7598  theyll ban donald trump to virtue signal thoug...   \n",
       "\n",
       "                                           msg_tokenied  \\\n",
       "0     [guardian, balls, ignore, media, ban, stopped,...   \n",
       "1     [tell, permanent, fire, ban, helps, anyone, wo...   \n",
       "2                   [fucking, blanket, ban, celebs, tv]   \n",
       "3     [scott, üôè, tweets, andrew, yang, im, formal, s...   \n",
       "4     [misleading, information, sometimes, close, fr...   \n",
       "...                                                 ...   \n",
       "7594  [learn, viruses, vaccines, social, media, cens...   \n",
       "7595  [editorial, say, trudeaus, online, censorship,...   \n",
       "7596  [hate, trump, least, aint, try, ban, backwoods...   \n",
       "7597  [please, help, support, uswe, biggest, victims...   \n",
       "7598  [theyll, ban, donald, trump, virtue, signal, t...   \n",
       "\n",
       "                                            msg_stemmed  \\\n",
       "0     [guardian, ball, ignor, media, ban, stop, shor...   \n",
       "1     [tell, perman, fire, ban, help, anyon, would, ...   \n",
       "2                       [fuck, blanket, ban, celeb, tv]   \n",
       "3     [scott, üôè, tweet, andrew, yang, im, formal, su...   \n",
       "4     [mislead, inform, sometim, close, free, though...   \n",
       "...                                                 ...   \n",
       "7594  [learn, virus, vaccin, social, media, censorsh...   \n",
       "7595   [editori, say, trudeau, onlin, censorship, plan]   \n",
       "7596  [hate, trump, least, aint, tri, ban, backwood,...   \n",
       "7597  [pleas, help, support, usw, biggest, victim, m...   \n",
       "7598  [theyll, ban, donald, trump, virtu, signal, th...   \n",
       "\n",
       "                                         msg_lemmatized  \n",
       "0     [guardian, ball, ignore, medium, ban, stopped,...  \n",
       "1     [tell, permanent, fire, ban, help, anyone, wou...  \n",
       "2                   [fucking, blanket, ban, celebs, tv]  \n",
       "3     [scott, üôè, tweet, andrew, yang, im, formal, su...  \n",
       "4     [misleading, information, sometimes, close, fr...  \n",
       "...                                                 ...  \n",
       "7594  [learn, virus, vaccine, social, medium, censor...  \n",
       "7595  [editorial, say, trudeaus, online, censorship,...  \n",
       "7596  [hate, trump, least, aint, try, ban, backwoods...  \n",
       "7597  [please, help, support, uswe, biggest, victim,...  \n",
       "7598  [theyll, ban, donald, trump, virtue, signal, t...  \n",
       "\n",
       "[7599 rows x 5 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "id_text['msg_lemmatized']=id_text['msg_tokenied'].apply(lambda x:lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to test and train set\n",
    "X = id_text.iloc[:, -1].values\n",
    "y = dataset.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i]=='Relevant':\n",
    "        y[i]=1\n",
    "    else:\n",
    "        y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['correlation',\n",
       " 'leftist',\n",
       " 'big',\n",
       " 'tech',\n",
       " 'shadow',\n",
       " 'banning',\n",
       " 'throttling',\n",
       " 'labeling',\n",
       " 'throwing',\n",
       " 'conservative',\n",
       " 'voice',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'election',\n",
       " 'capitol',\n",
       " 'protest',\n",
       " '500',\n",
       " 'trump',\n",
       " 'supporter',\n",
       " 'crashed',\n",
       " 'way',\n",
       " 'capitol']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-b437f8ad8c5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Train the model using the training sets y_pred=clf.predict(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# prediction on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    302\u001b[0m             )\n\u001b[0;32m    303\u001b[0m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[1;32m--> 304\u001b[1;33m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    804\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\users\\darya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# prediction on test set\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
